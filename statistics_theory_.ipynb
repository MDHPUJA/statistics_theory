{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNl3uaLJ2zsk3TG5O2SC/1I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDHPUJA/statistics_theory/blob/main/statistics_theory_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "fBkGCYXIIwKp",
        "outputId": "216833ca-d607-4303-b256-d7b9c5f8a4a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nKey Properties\\n(a) Non-Negativity\\nThe F-distribution is always non-negative because it is based on ratios of squared values (variances). This means:\\n\\n𝐹≥0\\nF≥0\\n(b) Shape\\n*The F-distribution is right-skewed, especially for smaller degrees of freedom.\\n*As the degrees of freedom 𝜈1 and 𝜈2 increase, the distribution becomes less skewed and approaches a normal distribution.\\n(c) Degrees of Freedom\\nThe shape of the F-distribution depends on two degrees of freedom:\\n*𝜈1(numerator degrees of freedom)\\n*ν2(denominator degrees of freedom)\\n\\n(d) Mean\\nThe mean of the F-distribution exists only when \\n𝜈2>2 and is given by:\\n\\n𝐸[𝐹]=𝜈2/(𝜈2−2)\\n\\n(e) Variance\\nThe variance of the F-distribution exists only when 𝜈2>4\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#QUESTION 1\n",
        "# Explain the properties of the F-distribution.\n",
        "'''\n",
        "Key Properties\n",
        "(a) Non-Negativity\n",
        "The F-distribution is always non-negative because it is based on ratios of squared values (variances). This means:\n",
        "\n",
        "𝐹≥0\n",
        "F≥0\n",
        "(b) Shape\n",
        "*The F-distribution is right-skewed, especially for smaller degrees of freedom.\n",
        "*As the degrees of freedom 𝜈1 and 𝜈2 increase, the distribution becomes less skewed and approaches a normal distribution.\n",
        "(c) Degrees of Freedom\n",
        "The shape of the F-distribution depends on two degrees of freedom:\n",
        "*𝜈1(numerator degrees of freedom)\n",
        "*ν2(denominator degrees of freedom)\n",
        "\n",
        "(d) Mean\n",
        "The mean of the F-distribution exists only when\n",
        "𝜈2>2 and is given by:\n",
        "\n",
        "𝐸[𝐹]=𝜈2/(𝜈2−2)\n",
        "\n",
        "(e) Variance\n",
        "The variance of the F-distribution exists only when 𝜈2>4\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 2\n",
        "# In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "'''\n",
        "The F-distribution is used in several statistical tests, primarily because it is\n",
        "designed to evaluate the ratio of variances. Its properties make it particularly\n",
        "suitable for comparing variance-based measures and assessing relationships between\n",
        "variables in various scenarios. Below are the types of statistical tests where the\n",
        "F-distribution is used and why it is appropriate for these tests:\n",
        "\n",
        "\n",
        "1. Analysis of Variance (ANOVA)\n",
        "-Purpose: To compare the means of three or more groups to determine if at least\n",
        "one group mean is significantly different from the others.\n",
        "- How the F-distribution is used:\n",
        "  - The F-statistic in ANOVA is calculated as the ratio of:\n",
        "    F = text{Variance between groups\\Variance within groups\n",
        "\n",
        "  - If the group means are all equal, the between-group variance will be small\n",
        "\n",
        "\n",
        "2. Regression Analysis\n",
        "- Purpose: To evaluate the overall significance of a regression model, determining\n",
        "whether the independent variables as a group predict the dependent variable.\n",
        "- How the F-distribution is used:\n",
        "  - The F-statistic tests the null hypothesis that all regression coefficients\n",
        "  (except the intercept) are zero, meaning the independent variables collectively\n",
        "  have no effect on the dependent variable.\n",
        "\n",
        "3. F-Test for Equality of Variances\n",
        "- Purpose: To compare the variances of two independent samples to determine if\n",
        "they are significantly different.\n",
        "- How the F-distribution is used:\n",
        "    (with the larger variance typically placed in the numerator to ensure \\( F \\geq 1 \\)).\n",
        "  - The test compares the calculated F-statistic to the critical value of the F-distribution with appropriate degrees of freedom.\n",
        "\n",
        "4. General Linear Model (GLM) Comparisons\n",
        "- Purpose: To compare nested models (e.g., a full model with more predictors against a reduced model with fewer predictors) in terms of their fit to the data.\n",
        "- How the F-distribution is used:\n",
        "  - The F-statistic compares the additional variance explained by the more complex model to the residual variance.\n",
        "  - This is particularly common in hypothesis tests for polynomial regression, multiple linear regression, and logistic regression.'''\n"
      ],
      "metadata": {
        "id": "JK3nTyQ2TkUy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "b4347e0d-356e-4035-a177-f9c475865e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe F-distribution is used in several statistical tests, primarily because it is \\ndesigned to evaluate the ratio of variances. Its properties make it particularly \\nsuitable for comparing variance-based measures and assessing relationships between \\nvariables in various scenarios. Below are the types of statistical tests where the \\nF-distribution is used and why it is appropriate for these tests:\\n\\n\\n1. Analysis of Variance (ANOVA)\\n-Purpose: To compare the means of three or more groups to determine if at least \\none group mean is significantly different from the others.\\n- How the F-distribution is used:\\n  - The F-statistic in ANOVA is calculated as the ratio of:\\n    F = text{Variance between groups\\\\Variance within groups\\n\\n  - If the group means are all equal, the between-group variance will be small \\n\\n\\n2. Regression Analysis\\n- Purpose: To evaluate the overall significance of a regression model, determining \\nwhether the independent variables as a group predict the dependent variable.\\n- How the F-distribution is used:\\n  - The F-statistic tests the null hypothesis that all regression coefficients \\n  (except the intercept) are zero, meaning the independent variables collectively \\n  have no effect on the dependent variable.\\n  \\n3. F-Test for Equality of Variances\\n- Purpose: To compare the variances of two independent samples to determine if \\nthey are significantly different.\\n- How the F-distribution is used:\\n    (with the larger variance typically placed in the numerator to ensure \\\\( F \\\\geq 1 \\\\)).\\n  - The test compares the calculated F-statistic to the critical value of the F-distribution with appropriate degrees of freedom.\\n\\n4. General Linear Model (GLM) Comparisons\\n- Purpose: To compare nested models (e.g., a full model with more predictors against a reduced model with fewer predictors) in terms of their fit to the data.\\n- How the F-distribution is used:\\n  - The F-statistic compares the additional variance explained by the more complex model to the residual variance.\\n  - This is particularly common in hypothesis tests for polynomial regression, multiple linear regression, and logistic regression.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 3\n",
        "#What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
        "'''\n",
        "Key Assumptions for an F-Test\n",
        "1. Independence of Observations\n",
        "The observations within each sample must be independent.\n",
        "The two samples must also be independent of each other.\n",
        "Why important: Independence ensures that the variances being compared are not\n",
        "influenced by any dependencies, which could distort the variance estimates.\n",
        "2. Normality of Populations\n",
        "Both populations from which the samples are drawn must follow a normal distribution.\n",
        "Why important: The F-test assumes that the ratio of sample variances follows an\n",
        "F-distribution, which is only valid when the populations are normally distributed.\n",
        "3. Random Sampling\n",
        "The samples must be randomly drawn from their respective populations.\n",
        "Why important: Random sampling reduces bias and ensures that the samples are\n",
        "representative of their populations.\n",
        "4. Positive Variances\n",
        "The variances of both samples must be positive (i.e., not zero).\n",
        "Why important: Variances represent squared deviations from the mean, so they\n",
        "must be non-negative. An F-statistic cannot be computed if either variance is zero.\n",
        "5. Homogeneity of Sample Sizes (Optional but Preferred)\n",
        "While not strictly required, the test performs better when the sample sizes of\n",
        "the two groups are similar.\n",
        "Why important: Unequal sample sizes can lead to instability in the F-ratio,\n",
        "especially if variances differ significantly.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "kH5IXZGCbdh_",
        "outputId": "2b8cf892-0d84-40a5-f5d6-d5a2a3b43e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nKey Assumptions for an F-Test\\n1. Independence of Observations\\nThe observations within each sample must be independent.\\nThe two samples must also be independent of each other.\\nWhy important: Independence ensures that the variances being compared are not \\ninfluenced by any dependencies, which could distort the variance estimates.\\n2. Normality of Populations\\nBoth populations from which the samples are drawn must follow a normal distribution.\\nWhy important: The F-test assumes that the ratio of sample variances follows an \\nF-distribution, which is only valid when the populations are normally distributed.\\n3. Random Sampling\\nThe samples must be randomly drawn from their respective populations.\\nWhy important: Random sampling reduces bias and ensures that the samples are \\nrepresentative of their populations.\\n4. Positive Variances\\nThe variances of both samples must be positive (i.e., not zero).\\nWhy important: Variances represent squared deviations from the mean, so they \\nmust be non-negative. An F-statistic cannot be computed if either variance is zero.\\n5. Homogeneity of Sample Sizes (Optional but Preferred)\\nWhile not strictly required, the test performs better when the sample sizes of \\nthe two groups are similar.\\nWhy important: Unequal sample sizes can lead to instability in the F-ratio, \\nespecially if variances differ significantly.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 4\n",
        "#What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "'''\n",
        "Purpose of ANOVA:\n",
        "Analysis of Variance (ANOVA) is used to determine whether there are statistically\n",
        "significant differences among the means of three or more groups. It works by\n",
        "comparing the variability between group means to the variability within the groups.\n",
        "The goal is to assess whether the differences between group means are greater than\n",
        "what could be expected by chance.\n",
        "\n",
        "The null hypothesis in ANOVA states that all group means are equal, while the\n",
        "alternative hypothesis suggests that at least one group mean is different. The\n",
        "test calculates an F-statistic, which is the ratio of the variance between the\n",
        "group means to the variance within the groups. If the F-statistic is large, it\n",
        "indicates that the group means are likely to differ significantly.\n",
        "\n",
        "Purpose of a t-Test:\n",
        "The t-test is used to compare the means of two groups. It assesses whether the\n",
        "observed difference between the two group means is statistically significant or\n",
        "could have occurred by chance. There are different types of t-tests, such as the\n",
        "independent t-test for comparing two unrelated groups and the paired t-test for\n",
        "comparing two related groups.\n",
        "\n",
        "The null hypothesis in a t-test posits that the two means are equal, while the\n",
        "alternative hypothesis suggests they are different. The t-test uses a t-statistic\n",
        "to evaluate the difference between the means relative to the variability within\n",
        "the data.\n",
        "\n",
        "Key Differences Between ANOVA and a t-Test:\n",
        "The primary difference lies in the number of groups being compared. ANOVA is\n",
        "designed to handle comparisons between three or more groups, while the t-test is\n",
        "limited to two groups. ANOVA is a more general approach that works with multiple\n",
        "groups simultaneously.\n",
        "\n",
        "Another major distinction is how the two methods control for statistical error.\n",
        "If multiple t-tests are conducted to compare several group pairs, the risk of a\n",
        "Type I error (false positive) increases with each additional test. ANOVA mitigates\n",
        "this issue by testing all group means at once, thereby maintaining the overall error rate.\n",
        "\n",
        "Finally, when ANOVA detects significant differences among groups, additional\n",
        "post-hoc tests are required to pinpoint which specific groups differ.\n",
        "In contrast, the t-test directly provides results for the two groups being\n",
        "compared, so no further analysis is necessary.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "WDNQAdI4ckBT",
        "outputId": "0d4b6f72-49e8-43e6-9f4d-1354be2f95e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPurpose of ANOVA:\\nAnalysis of Variance (ANOVA) is used to determine whether there are statistically \\nsignificant differences among the means of three or more groups. It works by \\ncomparing the variability between group means to the variability within the groups. \\nThe goal is to assess whether the differences between group means are greater than \\nwhat could be expected by chance.\\n\\nThe null hypothesis in ANOVA states that all group means are equal, while the \\nalternative hypothesis suggests that at least one group mean is different. The \\ntest calculates an F-statistic, which is the ratio of the variance between the \\ngroup means to the variance within the groups. If the F-statistic is large, it \\nindicates that the group means are likely to differ significantly.\\n\\nPurpose of a t-Test:\\nThe t-test is used to compare the means of two groups. It assesses whether the \\nobserved difference between the two group means is statistically significant or \\ncould have occurred by chance. There are different types of t-tests, such as the \\nindependent t-test for comparing two unrelated groups and the paired t-test for \\ncomparing two related groups.\\n\\nThe null hypothesis in a t-test posits that the two means are equal, while the \\nalternative hypothesis suggests they are different. The t-test uses a t-statistic \\nto evaluate the difference between the means relative to the variability within \\nthe data.\\n\\nKey Differences Between ANOVA and a t-Test:\\nThe primary difference lies in the number of groups being compared. ANOVA is \\ndesigned to handle comparisons between three or more groups, while the t-test is \\nlimited to two groups. ANOVA is a more general approach that works with multiple \\ngroups simultaneously.\\n\\nAnother major distinction is how the two methods control for statistical error. \\nIf multiple t-tests are conducted to compare several group pairs, the risk of a \\nType I error (false positive) increases with each additional test. ANOVA mitigates \\nthis issue by testing all group means at once, thereby maintaining the overall error rate.\\n\\nFinally, when ANOVA detects significant differences among groups, additional \\npost-hoc tests are required to pinpoint which specific groups differ. \\nIn contrast, the t-test directly provides results for the two groups being \\ncompared, so no further analysis is necessary.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 5\n",
        "#  Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups\n",
        "'''\n",
        "When and Why to Use One-Way ANOVA Instead of Multiple t-Tests\n",
        "When comparing the means of more than two groups, a one-way ANOVA is preferred\n",
        "over conducting multiple t-tests for several important reasons.\n",
        "\n",
        "When to Use a One-Way ANOVA\n",
        "A one-way ANOVA is appropriate when:\n",
        "\n",
        "Three or More Groups: You have three or more groups or categories and want to\n",
        "determine whether their means differ significantly.\n",
        "Example: Comparing the test scores of students from three different teaching methods.\n",
        "Single Independent Variable: The groups are differentiated by a single factor or\n",
        "independent variable (e.g., teaching method).\n",
        "Continuous Dependent Variable: The data being compared (e.g., test scores) is continuous.\n",
        "Assumptions Are Met: The data meets ANOVA assumptions, such as:\n",
        "Independence of observations.\n",
        "Normal distribution of the dependent variable within each group.\n",
        "Homogeneity of variances across groups.\n",
        "Why Use One-Way ANOVA Instead of Multiple t-Tests\n",
        "Avoiding Increased Risk of Type I Error:\n",
        "\n",
        "Each t-test you conduct comes with a risk of a Type I error (false positive).\n",
        "For example, if you conduct three pairwise t-tests with a significance level (α)\n",
        "of 0.05, the cumulative probability of making at least one Type I error increases.\n",
        "With one-way ANOVA, all group comparisons are tested simultaneously, and the\n",
        "overall significance level is maintained at α=0.05.\n",
        "Efficiency and Simplicity:\n",
        "\n",
        "Conducting multiple t-tests for several groups can be cumbersome and\n",
        "computationally inefficient. One-way ANOVA allows you to test all groups in a\n",
        "single analysis.\n",
        "ANOVA provides a single F-statistic to determine if any group mean is significantly\n",
        "different, simplifying the interpretation.\n",
        "Holistic Analysis:\n",
        "\n",
        "Multiple t-tests only compare two groups at a time, without considering the\n",
        "overall variance across all groups. ANOVA partitions total variance into\n",
        "within-group variance and between-group variance, providing a broader view of the data.\n",
        "Post-Hoc Testing:\n",
        "\n",
        "If the one-way ANOVA reveals significant differences among group means, post-hoc\n",
        "tests (e.g., Tukey's HSD) can identify specifically which groups differ. This\n",
        "structured approach avoids redundancy and maintains statistical rigor.\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "qkbd3GS6epzn",
        "outputId": "421cf0a9-45aa-48ef-d8ca-10ff1af5ae18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nWhen and Why to Use One-Way ANOVA Instead of Multiple t-Tests\\nWhen comparing the means of more than two groups, a one-way ANOVA is preferred \\nover conducting multiple t-tests for several important reasons.\\n\\nWhen to Use a One-Way ANOVA\\nA one-way ANOVA is appropriate when:\\n\\nThree or More Groups: You have three or more groups or categories and want to \\ndetermine whether their means differ significantly.\\nExample: Comparing the test scores of students from three different teaching methods.\\nSingle Independent Variable: The groups are differentiated by a single factor or \\nindependent variable (e.g., teaching method).\\nContinuous Dependent Variable: The data being compared (e.g., test scores) is continuous.\\nAssumptions Are Met: The data meets ANOVA assumptions, such as:\\nIndependence of observations.\\nNormal distribution of the dependent variable within each group.\\nHomogeneity of variances across groups.\\nWhy Use One-Way ANOVA Instead of Multiple t-Tests\\nAvoiding Increased Risk of Type I Error:\\n\\nEach t-test you conduct comes with a risk of a Type I error (false positive). \\nFor example, if you conduct three pairwise t-tests with a significance level (α) \\nof 0.05, the cumulative probability of making at least one Type I error increases.\\nWith one-way ANOVA, all group comparisons are tested simultaneously, and the \\noverall significance level is maintained at α=0.05.\\nEfficiency and Simplicity:\\n\\nConducting multiple t-tests for several groups can be cumbersome and \\ncomputationally inefficient. One-way ANOVA allows you to test all groups in a \\nsingle analysis.\\nANOVA provides a single F-statistic to determine if any group mean is significantly \\ndifferent, simplifying the interpretation.\\nHolistic Analysis:\\n\\nMultiple t-tests only compare two groups at a time, without considering the \\noverall variance across all groups. ANOVA partitions total variance into \\nwithin-group variance and between-group variance, providing a broader view of the data.\\nPost-Hoc Testing:\\n\\nIf the one-way ANOVA reveals significant differences among group means, post-hoc \\ntests (e.g., Tukey's HSD) can identify specifically which groups differ. This \\nstructured approach avoids redundancy and maintains statistical rigor.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 6\n",
        "# Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?\n",
        "'''\n",
        "Partitioning of Variance in ANOVA\n",
        "\n",
        "In Analysis of Variance (ANOVA), the total variance in the data is divided into\n",
        "two components: between-group variance and within-group variance. This partitioning\n",
        "is central to understanding how differences among group means are assessed relative\n",
        "to the variability within the groups.\n",
        "\n",
        "1. Total Variance (SS_Total)\n",
        "The total variance measures the overall variability in the data, regardless of group membership.\n",
        "It is calculated as the sum of squared deviations of each observation from the grand mean\n",
        "\n",
        "2. Between-Group Variance (SS_Between)\n",
        "This component represents the variability between the group means and the grand\n",
        "mean. It captures how much the group means differ from each other.\n",
        "\n",
        "The larger the between-group variance, the greater the differences between the\n",
        "group means, indicating a potential effect of the independent variable.\n",
        "\n",
        "\n",
        "3. Within-Group Variance (SS_ext{Within)\n",
        "This component represents the variability within each group around the group mean.\n",
        "It captures the random variation in the data that is not explained by group\n",
        "membership.\n",
        "\n",
        "Relationship Between Components\n",
        "The total variance is the sum of the between-group and within-group variances:\n",
        "\n",
        "SS_{Total} = SS_{Between} + SS_{Within}\n",
        "\n",
        "\n",
        "\n",
        "Calculation of the F-Statistic\n",
        "The F-statistic compares the variance explained by the group\n",
        "differences (between-group variance) to the variance within the\n",
        "groups (within-group variance).\n",
        "\n",
        "A large F-statistic indicates that the variance between the group means is\n",
        "significantly larger than the variance within the groups, suggesting that at\n",
        "least one group mean is different from the others.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "qgtRMB8Zfwag",
        "outputId": "c828f7c2-cc3d-4e38-921d-8c90e44e5805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPartitioning of Variance in ANOVA\\n\\nIn Analysis of Variance (ANOVA), the total variance in the data is divided into \\ntwo components: between-group variance and within-group variance. This partitioning \\nis central to understanding how differences among group means are assessed relative \\nto the variability within the groups.\\n\\n1. Total Variance (SS_Total)\\nThe total variance measures the overall variability in the data, regardless of group membership. \\nIt is calculated as the sum of squared deviations of each observation from the grand mean \\n\\n2. Between-Group Variance (SS_Between)\\nThis component represents the variability between the group means and the grand \\nmean. It captures how much the group means differ from each other. \\n\\nThe larger the between-group variance, the greater the differences between the \\ngroup means, indicating a potential effect of the independent variable.\\n\\n\\n3. Within-Group Variance (SS_ext{Within)\\nThis component represents the variability within each group around the group mean. \\nIt captures the random variation in the data that is not explained by group \\nmembership.\\n\\nRelationship Between Components\\nThe total variance is the sum of the between-group and within-group variances:\\n\\nSS_{Total} = SS_{Between} + SS_{Within}\\n\\n\\n\\nCalculation of the F-Statistic\\nThe F-statistic compares the variance explained by the group \\ndifferences (between-group variance) to the variance within the \\ngroups (within-group variance).\\n\\nA large F-statistic indicates that the variance between the group means is \\nsignificantly larger than the variance within the groups, suggesting that at \\nleast one group mean is different from the others.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 7\n",
        "#  Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "'''\n",
        " Comparison of Classical (Frequentist) and Bayesian Approaches to ANOVA\n",
        "\n",
        "The classical (frequentist) ANOVA and the Bayesian ANOVA differ fundamentally in\n",
        "their philosophies and methodologies for analyzing variance. These differences\n",
        "affect how they handle uncertainty, parameter estimation, and hypothesis testing.\n",
        "\n",
        "---\n",
        "\n",
        "1. Philosophical Framework\n",
        "- Classical (Frequentist) ANOVA:\n",
        "  - Relies on the concept of long-run frequencies of events.\n",
        "  - Assumes fixed parameters (e.g., group means, variances) and focuses on\n",
        "  testing hypotheses about them.\n",
        "  - Uncertainty is quantified using p-values, which indicate how extreme the\n",
        "  observed data is under the null hypothesis.\n",
        "\n",
        "- Bayesian ANOVA:\n",
        "  - Based on Bayes' theorem, which incorporates prior beliefs about parameters\n",
        "  and updates these beliefs based on observed data.\n",
        "  - Parameters are treated as random variables with probability distributions.\n",
        "  - Uncertainty is quantified using posterior distributions of the parameters.\n",
        "\n",
        "---\n",
        "\n",
        "2. Handling Uncertainty\n",
        "- Classical ANOVA:\n",
        "  - Assumes data variability arises from sampling variability around fixed parameters.\n",
        "  - Uses confidence intervals to express the range within which a parameter is\n",
        "    likely to lie, assuming repeated sampling.\n",
        "  - Does not incorporate prior information about parameters.\n",
        "\n",
        "- Bayesian ANOVA:\n",
        "  - Models uncertainty explicitly by computing the posterior distributions of\n",
        "    parameters (e.g., group means, variances).\n",
        "  - Incorporates prior knowledge or beliefs through prior distributions, which\n",
        "    can influence results, especially with limited data.\n",
        "  - Provides a full probabilistic description of uncertainty, offering probabilities\n",
        "    for hypotheses directly (e.g., the probability\n",
        "   that one group mean is greater than another).\n",
        "\n",
        "---\n",
        "\n",
        "3. Parameter Estimation\n",
        "-   Classical ANOVA:\n",
        "  - Estimates parameters (e.g., group means, variances) using point estimates,\n",
        "    such as sample means and variances.\n",
        "  - Assumes these estimates are unbiased and interprets them in the context of\n",
        "    sampling distributions.\n",
        "\n",
        "- Bayesian ANOVA:\n",
        "  - Produces posterior distributions for parameters, allowing for probabilistic\n",
        "    inferences.\n",
        "  - Can incorporate uncertainty in the estimates through credible intervals\n",
        "    (e.g., a 95% credible interval is the range where the parameter lies with\n",
        "    95% probability given the data and prior).\n",
        "\n",
        "4. Hypothesis Testing\n",
        "  - Classical ANOVA:\n",
        "  - Tests hypotheses using p-values and the F-statistic.\n",
        "  - Null hypothesis (H_0): All group means are equal (mu_1 = mu_2 = mu_3 = ......)).\n",
        "  - Rejects (H_0) if the p-value is below a predefined threshold (e.g., alpha = 0.05 ).\n",
        "  - Does not provide the probability of (H_0) being true, only whether the data are extreme under (H_0).\n",
        "\n",
        "-  Bayesian ANOVA:\n",
        "  - Evaluates hypotheses by comparing their posterior probabilities or Bayes factors.\n",
        "  - Provides a direct probability statement about hypotheses, such as \"the probability that group A’s mean is higher than group B’s mean is 80%.\"\n",
        "  - Hypotheses are compared using **Bayes factors**, which quantify the evidence for one hypothesis over another.\n",
        "\n",
        "---\n",
        "\n",
        "    5. Flexibility\n",
        "-   Classical ANOVA:\n",
        "  - Rigid in its framework; cannot easily incorporate prior knowledge or uncertainty about parameters.\n",
        "  - Requires assumptions like normality and equal variances to be met.\n",
        "\n",
        "-   Bayesian ANOVA:\n",
        "  - Highly flexible; can include prior distributions and handle complex models (e.g., hierarchical models).\n",
        "  - Adapts better to small sample sizes or non-standard data structures by leveraging prior information.\n",
        "\n",
        "---\n",
        "\n",
        " 6. Interpretation\n",
        "-   Classical ANOVA:\n",
        "  - Results are interpreted in terms of probabilities of observing data under\n",
        "    the null hypothesis.\n",
        "  - P-values and confidence intervals provide indirect measures of significance\n",
        "    and uncertainty.\n",
        "\n",
        "-   Bayesian ANOVA:\n",
        "  - Results are interpreted in terms of the probability of hypotheses or parameters being true, given the observed data.\n",
        "  - Credible intervals and posterior probabilities offer a more intuitive understanding of uncertainty.\n",
        "\n",
        "\n",
        "Choosing Between the Two\n",
        "-   Use classical ANOVA when:\n",
        "  - Data meets its assumptions (e.g., normality, equal variances).\n",
        "  - Prior information is unavailable or not relevant.\n",
        "  - Simplicity and computational efficiency are desired.\n",
        "\n",
        "- Use Bayesian ANOVA when:\n",
        "  - You want to incorporate prior information.\n",
        "  - Data is sparse, or assumptions like normality are violated.\n",
        "  - Probabilistic, intuitive interpretations are needed for decision-making.\n",
        "  '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "TfJK-6Y6jJIR",
        "outputId": "b2708fe6-9920-4c39-fa93-35d1750426f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n Comparison of Classical (Frequentist) and Bayesian Approaches to ANOVA\\n\\nThe classical (frequentist) ANOVA and the Bayesian ANOVA differ fundamentally in \\ntheir philosophies and methodologies for analyzing variance. These differences \\naffect how they handle uncertainty, parameter estimation, and hypothesis testing.\\n\\n---\\n\\n1. Philosophical Framework\\n- Classical (Frequentist) ANOVA:\\n  - Relies on the concept of long-run frequencies of events.\\n  - Assumes fixed parameters (e.g., group means, variances) and focuses on \\n  testing hypotheses about them.\\n  - Uncertainty is quantified using p-values, which indicate how extreme the \\n  observed data is under the null hypothesis.\\n\\n- Bayesian ANOVA:\\n  - Based on Bayes\\' theorem, which incorporates prior beliefs about parameters \\n  and updates these beliefs based on observed data.\\n  - Parameters are treated as random variables with probability distributions.\\n  - Uncertainty is quantified using posterior distributions of the parameters.\\n\\n---\\n\\n2. Handling Uncertainty\\n- Classical ANOVA:\\n  - Assumes data variability arises from sampling variability around fixed parameters.\\n  - Uses confidence intervals to express the range within which a parameter is \\n    likely to lie, assuming repeated sampling.\\n  - Does not incorporate prior information about parameters.\\n  \\n- Bayesian ANOVA:\\n  - Models uncertainty explicitly by computing the posterior distributions of \\n    parameters (e.g., group means, variances).\\n  - Incorporates prior knowledge or beliefs through prior distributions, which\\n    can influence results, especially with limited data.\\n  - Provides a full probabilistic description of uncertainty, offering probabilities \\n    for hypotheses directly (e.g., the probability\\n   that one group mean is greater than another).\\n\\n---\\n\\n3. Parameter Estimation\\n-   Classical ANOVA:\\n  - Estimates parameters (e.g., group means, variances) using point estimates, \\n    such as sample means and variances.\\n  - Assumes these estimates are unbiased and interprets them in the context of \\n    sampling distributions.\\n  \\n- Bayesian ANOVA:\\n  - Produces posterior distributions for parameters, allowing for probabilistic \\n    inferences.\\n  - Can incorporate uncertainty in the estimates through credible intervals \\n    (e.g., a 95% credible interval is the range where the parameter lies with \\n    95% probability given the data and prior).\\n\\n4. Hypothesis Testing\\n  - Classical ANOVA:\\n  - Tests hypotheses using p-values and the F-statistic.\\n  - Null hypothesis (H_0): All group means are equal (mu_1 = mu_2 = mu_3 = ......)).\\n  - Rejects (H_0) if the p-value is below a predefined threshold (e.g., alpha = 0.05 ).\\n  - Does not provide the probability of (H_0) being true, only whether the data are extreme under (H_0).\\n\\n-  Bayesian ANOVA:\\n  - Evaluates hypotheses by comparing their posterior probabilities or Bayes factors.\\n  - Provides a direct probability statement about hypotheses, such as \"the probability that group A’s mean is higher than group B’s mean is 80%.\"\\n  - Hypotheses are compared using **Bayes factors**, which quantify the evidence for one hypothesis over another.\\n\\n---\\n\\n    5. Flexibility\\n-   Classical ANOVA:\\n  - Rigid in its framework; cannot easily incorporate prior knowledge or uncertainty about parameters.\\n  - Requires assumptions like normality and equal variances to be met.\\n  \\n-   Bayesian ANOVA:\\n  - Highly flexible; can include prior distributions and handle complex models (e.g., hierarchical models).\\n  - Adapts better to small sample sizes or non-standard data structures by leveraging prior information.\\n\\n---\\n\\n 6. Interpretation\\n-   Classical ANOVA:\\n  - Results are interpreted in terms of probabilities of observing data under \\n    the null hypothesis.\\n  - P-values and confidence intervals provide indirect measures of significance \\n    and uncertainty.\\n\\n-   Bayesian ANOVA:\\n  - Results are interpreted in terms of the probability of hypotheses or parameters being true, given the observed data.\\n  - Credible intervals and posterior probabilities offer a more intuitive understanding of uncertainty.\\n\\n\\nChoosing Between the Two\\n-   Use classical ANOVA when:\\n  - Data meets its assumptions (e.g., normality, equal variances).\\n  - Prior information is unavailable or not relevant.\\n  - Simplicity and computational efficiency are desired.\\n  \\n- Use Bayesian ANOVA when:\\n  - You want to incorporate prior information.\\n  - Data is sparse, or assumptions like normality are violated.\\n  - Probabilistic, intuitive interpretations are needed for decision-making.\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION 8\n",
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Datasets\n",
        "profession_A = [48, 52, 55, 60, 62]\n",
        "profession_B = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Step 1: Calculate sample variances\n",
        "var_A = np.var(profession_A, ddof=1)  # Variance of Profession_A\n",
        "var_B = np.var(profession_B, ddof=1)  # Variance of Profession_B\n",
        "\n",
        "# Step 2: Compute the F-statistic\n",
        "F_statistic = var_A / var_B if var_A > var_B else var_B / var_A\n",
        "\n",
        "# Step 3: Degrees of freedom\n",
        "df_A = len(profession_A) - 1\n",
        "df_B = len(profession_B) - 1\n",
        "\n",
        "# Step 4: Compute the p-value\n",
        "p_value = 2 * min(f.cdf(F_statistic, df_A, df_B), 1 - f.cdf(F_statistic, df_A, df_B))\n",
        "\n",
        "# Step 5: Output results\n",
        "print(f\"Variance of Profession_A: {var_A}\")\n",
        "print(f\"Variance of Profession_B: {var_B}\")\n",
        "print(f\"F-statistic: {F_statistic}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: Variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: Variances are not significantly different.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0OtMJTMquFl",
        "outputId": "9e013c5c-417d-4c19-bd81-a65266a17509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of Profession_A: 32.8\n",
            "Variance of Profession_B: 15.7\n",
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.49304859900533904\n",
            "Fail to reject the null hypothesis: Variances are not significantly different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for each region\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "# Output the results\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Interpretation of the p-value\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There are statistically significant differences between the regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There are no statistically significant differences between the regions.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUF33GEeyEWR",
        "outputId": "85fff135-97b2-4965-dedd-8eb22c160c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "P-value: 2.870664187937026e-07\n",
            "Reject the null hypothesis: There are statistically significant differences between the regions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYDHQswzye4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}